# TODOs

- [ ] Add references to the binaries in dataset.

## Intro / Background

Binary code similarity detection (BCSD) is becoming increasingly important as the rate of production and modularity of software grows.
Modern software is almost never written from scratch and is increasingly reliant on external libraries. For reverse engineers,
reducing the amount of repetitive assembly functions to analyze is important, as it allows them to be more efficient and focus on the
custom parts of a binary. When libraries are statically linked to a binary, it is helpful to quickly identify which functions are part
of common external libraries and which ones have never been seen before. Furthermore, if a vulnerability is found in a library,
it is important to efficiently determine whether another binary or firmware is using the vulnerable library,
so that its impact can be mitigated.

This work presents a method to effectively find binary clones across binaries using large language models (LLM). The method
is simpler and requires no training nor fine-tuning and matches state-of-the-art results. It has the
unique advantage of generating human interpretable feature vectors instead of numerical values. Additionally,
it effectively scales with the performance and size of the LLM used, and thus benefits from the ample amount of research
on large language models.

### Contributions

an elementary approach to BCSD purely based on the recent advancements in large language models (LLM) is developed.

Instead of generating a numerical feature vector for a given assembly function, we generate a feature set containing
human readable elements.

TBD: how does this method compare to SOTA?

Advantages:

TBD: We show that this method scales with the capabilities of large language models. 
    (This is great since significant investments & research goes to LLMs, blah blah)
- Scales very well with the capabilities of LLMs.
- No pre-training, no fine tuning, works for any LLM out of the box.

Humans can verify the feature set generated by our method.
    (In an ideal world, we would not need human verification. Current methods are not perfect so human verif is still needed.
     Our method eases this process and can more easily be debugged.)

## Related Work

### Static Analysis

Traditional methods make use of static analysis to detect clone assembly routines. With these methods, a trade-off has
to be made between the robustness to obfuscation and architecture differences, and the performance of the algorithm. [1]
Control flow graph analysis and comparison [3, 4] is known to be very robust to syntactic differences, but often involves "NP-hard"
problems. Other algorithms that use heuristics such as instruction frequency, longest-common-subsequence, or hashes [5, 6, 7] are more
efficient, but tend to fixate on the syntactic elements and their ordering rather than the semantics.

### Dynamic Analysis

Dynamic analysis consists of analyzing the features of a binary or code fragment by monitoring its runtime behavior. For BCSD
this method is compute intensive and requires a cross-platform emulator, but completely sidesteps the syntactic aspects of binary code
and solely analyzes the semantics. [2] As such, this method is highly resilient to obfuscations, but requires a sandboxed environment
equiped with complex software.

### Machine Learning Methods

The surge of interest and applications for machine learning in recent years has also affected BCSD.
Most state-of-the-art methods use natural language processing (NLP) to achieve their results [refs].
Notably, recent machine learning approaches try to incorporate the transformer architecture into BCSD tasks [refs].

TODO: Complete this with the methods we are comparing against.

- Others make use of widely available LLMs either during training as data annotation [ref] (CLAP)

## Methodology

### Overview

This method queries an LLM with an assembly function, asking it to analyze the function.
The generated response follows a specified JSON schema. 
To test for the similarity of two assembly functions, the responses generated by the LLM are compared,
generating a numerical index of similarity.

### Comparison

Machine learning based methods of BCSD typically generate a numerical vector embedding for each assembly function [refs], and then compare these vectors
using numerical methods such as cosine similarity. This method is different, because it generates a JSON object instead of a vector. This has
the unique advantage of being human interpretable, and thus easier to verify. For example, when matching a binary function against a database,
one can base themself on the human readable LLM output to verify that the analysis was done correctly and to assess whether the matched function
is indeed a clone.

Since our generated analysis is not numerical, we use an alternative method to compare two assembly functions. We interpret the JSON structure
as a tree, where booleans, numbers, and string elements are leaves, and non-empty lists and objects fields are nodes. We form a set containing all
root-to-leaf paths, and use jaccard similarity (Intesection over union) to obtain a similarity measure.

QUESTION: Do we need refs for cosine similarity & jaccard?

### Dataset

The dataset is composed of 7 binaries: busybox, coreutils, curl, image-magick, openssl, putty, and sqlite3.
All were compiled using gcc for the following platforms: x86_64, x86_32, arm, mips, powerpc.
For each binary and platform, binary objects were generated for all optimization levels (O0 to O3),
stripped of debug symbols. In total, yeilds 140 different binaries to analyze.
The binaries were dissassembled using IDA Pro, yielding 383_658 assembly functions.
Functions consisting of less than 3 instructions were not included as part of the dataset.
Pairs of equivalent functions from the same platform but distinct optimization level were made for cross optimization
evaluation, and pairs from the same optimization level but different platform were formed for cross
platform evaluation.

### Model

These local models are evaluated on our dataset.

- Qwen2.5-Coder [ref] with sizes 0.5B to 7B
- Qwen3 [ref] with sizes 0.6 to 4B
- Gemma-3n [ref] with sizes 0.5B to 4B

Most evaluations and tests were run using Qwen2.5-Coder, and we use this model as a baseline.
On all local models, the input context size was limited to 4096 tokens, and output tokens generation to 512.
Large assembly functions that did not fit within the input tokens were truncated.

For ablation study, the following commercial models were evaluated on our dataset.

- OpenAI's GPT-4.1-mini [ref]
- OpenAI's o4-mini [ref]
- Google's gemini-2.5-flash-lite [ref]

### Prompt

- What are the features we extract?
- What type of output do we expect (main different between prompts)?
    - How do we solve repetition.
    - How do we ensure a valid json output?

### Evaluation method

The mean reciprocal rank (MRR) and first position recall (Recall@1) metrics are used for evaluation and comparison to other methods.
A pool of 1_000 function pairs is used for evaluation. For each pair, we compare the generated features for the first element of the pair
with all second elements of the pairs contained in the pool. We rank the comparisons in order of most similar to least similar, and
calculate the metrics based on this ranking.

## Results

- Show results on cross-optimization and cross-architecture (Many different models ran locally).
- Ablation on the number of examples provided.
- Ablation on the model size.
- Ablation on the prompt used, examples of output.
- Show results on commercial models
- Token usage on commercial models

### Human interpretability

Our method offers a distinct advantage over the current state-of-the art, because the feature vectors generated are human interpretable.
Other machine learning based methods generate numerical vector embeddings and compare the vectors using numerical methods such as cosine similarity.

## Future Research

These results open avenues for further investigation in LLM based BCSD, and more broadly in LLM assisted reverse engineering.
A few of these oportunities are outlined here.

### Distillation & Fine Tuning

It was shown that commercial LLMs perform better than smaller local models. Using the process of distillation [ref] on smaller models
will likely cause large gains for these models, approaching the performance of foundational LLMs. Fine Tuning is another opportunity
that should be explored to determine whether it provides meaningful gains in performance. One method would be using open source software
as fine tuning data. One could use source code and generate a feature vector (either manually or by prompting a large language model),
and then use this analysis as baseline to fine tune the analysis of the compiled assembly code.

### LLM Reasoning

Some state of the art LLMs provide reasoning capabilities by having the model generate a chain of thought [8] it its response.
It has been shown that this type of output produces more accurate and higher quality responses [8, 9, 10]. We believe that similar
gains could be seen in assembly function analysis, at the cost of more output token generation and thus more compute.

# Refs

1. [A Survey of Binary Code Similarity Detection Techniques](https://www.mdpi.com/2079-9292/13/9/1715)
2. [Binary Code Similiarity Detection](https://ieeexplore.ieee.org/document/9678518)
3. [Scalable Graph-based Bug Search for Firmware Images](https://dl.acm.org/doi/10.1145/2976749.2978370)
4. [Graph-based comparison of Executable Objects](https://www.semanticscholar.org/paper/Graph-based-comparison-of-Executable-Objects-Dullien/7661d4110ef24dea74190f4af69bd206d6253db9)

5. [Detecting Clones Across Microsoft .NET Programming Languages](https://ieeexplore.ieee.org/document/6385136)
6. [Idea: Opcode-Sequence-Based Malware Detection](https://link.springer.com/chapter/10.1007/978-3-642-11747-3_3)
7. [Binary Function Clustering Using Semantic Hashes](https://ieeexplore.ieee.org/document/6406693)

8. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
9. [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916)
10. [THINKING LLMS: GENERAL INSTRUCTION FOLLOWING WITH THOUGHT GENERATION](https://arxiv.org/pdf/2410.10630)

- [Blanket execution: Dynamic similarity testing for program binaries and components](https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/egele)
- [BinSim: Trace-based Semantic Binary Diffing via System Call Sliced Segment Equivalence Checking](https://www.usenix.org/system/files/conference/usenixsecurity17/sec17-ming.pdf)
