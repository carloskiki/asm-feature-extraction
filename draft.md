## Intro

Modern software is increasingly reliant on external libraries.

For security researchers, detecting whether a binary uses some vulnerable library function is crucial to assess
and mitigate vulnerability exposure [1, 2]. For reverse engineers, reducing the amount of repetitive assembly functions
to analyze means being more productive, and allows them to focus on the custom parts of a binary.  Binary code similarity
detection (BCSD) is a binary analysis task that tries to solve these problems, by determining whether two compiled
code fragments behave in similar ways. This task is becoming increasingly important as the size, modularity and rate of
production of software grows.  For instance, when libraries are statically linked to a binary, BCSD can help to
quickly identify which functions are part of common external libraries and which ones have never been seen before.
Furthermore, if a vulnerability is found in a library, BCSD can be used to efficiently determine whether a proprietary
binary or firmware is using the vulnerable library.

Early approaches to BCSD used human defined heuristics to extract a "feature vector" from a binary function. These heuristics
could be calculated by statically examining the functions and its control flow graph (CFG), or could be measured at runtime
by executing the function in an emulated environment. These methods were deterministic and had the benefit of producing
human understandable feature vectors, but were often too simplistic or sometimes used computationally intractable alogrithms in
the case of CFG analysis.

More recently, machine learning (ML) based methods have shown to be better performing.
These methods work by producing an embedding for a binary function using techniques coming from natural language processing.
The generated embeddings are usually floating point tensors and serve as the "feature vector". These vectors are compared
with each other by using metrics such as cosine similarity.

Our work presents a method to effectively find binary clones across binaries using any pre-trained large language model (LLM).
The method is simpler than other ML approaches, requires no training nor fine-tuning, and matches state-of-the-art results.
It has the unique advantage of generating human interpretable feature vectors instead of numerical values. Additionally,
it effectively scales with the performance and size of the LLM used, and thus benefits from the ample amount of research
on in this area.

TODO: Outline challenges and how we address them.

// Our work tackles the problem of classifying assembly functions by their semantic behavior. We do so by generating
// a JSON structure that identifies a function by its behavior. This structure is the human interpretable version
// of a function, and is the equivalent of a embedding generated by large

### Contributions

- We provide an elementary approach to BCSD purely based on the recent advancements in large language models (LLM) that
    is effective at cross-optimization, cross-architecture, and cross-obfuscation retrieval. This method requires no
    pre-training, and generates human interpretable feature sets.
- We show that our approach scales with the performance and size of the LLM used, and benefits from new
    methods such as "reasoning" models.  (This is great since significant investments & research goes to LLMs, blah blah)
- We conduct experiments to demonstrate the capability of our technique and show that it performs as well as other state-of-the-art
    methods that require pre-training.

Instead of generating a numerical feature vector for a given assembly function, we generate a feature set containing
human readable elements.

TBD: how does this method compare to SOTA?

Advantages:

Humans can verify the feature set generated by our method.
    (In an ideal world, we would not need human verification. Current methods are not perfect so human verif is still needed.
     Our method eases this process and can more easily be debugged.)

## Background

Security researchers and reverse engineers routinely obtain unknown or proprietary executables. Reverse engineers try
to analyze these binaries to understand their underlying algorithms, while security researchers want to assess
the risk associated with potential vulnerabilities within the executable. This process is usually conducted using
a software reverse engineering platform such as Ghidra [ref] or IDA Pro [ref]. The main functions of these programs are to
disassemble and decompile a provided binary, so that its content can be analyzed by humans. Disassembly is
the process of retrieving the human-readable assembly instructions from the binary executable, whereas decompilation
is the process of generating higher-level pseudo-code from the instructions based on common patterns and heuristics.

BCSD is the process of determining whether two fragments of binary code perform similar actions.
These fragments are usually first disassembled, and are then compared for similarity. In practice,
similarity detection is performed with one known fragment (either because it was analyzed before
or because its source code is known), and one unknown fragment. If the unknown piece of code is deemed
highly similar to the known one, it greatly simplifies the analysis task, and reduces duplicate work.

### Problem Definition

Recurring terms:

- 
- Code fragment will be "function" from thereon.

- Definitions - define recuring terms in our research paper - Define the problem properly.
- Explain a bit of NLP, how other methods acheive their results vs. how we do it.

We use the term "assembly function" to denote the assembly code generated from compiling a function written in a
compiled language.

## Related Work

### Static Analysis

Traditional methods make use of static analysis to detect clone assembly routines. With these methods, a trade-off has
to be made between the robustness to obfuscation and architecture differences, and the performance of the algorithm. [1]
Control flow graph analysis and comparison [3, 4] is known to be robust to syntactic differences, but often involves computationally intractable
problems. Other algorithms that use heuristics such as instruction frequency, longest-common-subsequence, or hashes [5, 6, 7] are more
efficient, but tend to fixate on the syntactic elements and their ordering rather than the semantics.

### Dynamic Analysis

Dynamic analysis consists of analyzing the features of a binary or code fragment by monitoring its runtime behavior. For BCSD
this method is compute intensive and requires a cross-platform emulator, but completely sidesteps the syntactic aspects of binary code
and solely analyzes its semantics. [2] As such, this method is highly resilient to obfuscations, but requires a sandboxed environment
and is hard to generalize across architectures and application binary interfaces [ref].

### Machine Learning Methods

TODO: Missing specification of which is platform specific & requires pre-training

The surge of interest and applications for machine learning in recent years has also affected BCSD.
Most state-of-the-art methods use natural language processing (NLP) to achieve their results [refs].
Notably, recent machine learning approaches try to incorporate the transformer architecture into BCSD tasks [refs].

Asm2Vec [17] is one of the first methods to use a NLP based approach to tackle the BCSD problem. It interprets an
assembly function as a set of instruction sequences, where each instruction sequence is a possible execution path
of the function. It samples these sequences by randomly traversing the CFG of the assembly function, and then
uses a technique based on the PV-DM model [18] to generate an embedding for the assembly function. This solution
is not cross architecture compatible and requires pre-training.

SAFE [11] uses a process similar to Asm2Vec. It first encodes each instruction of an assembly function into a vector,
using the word2vec model [12]. Using a Self-Attentive Neural Network [13], SAFE then converts the sequence of instruction
vectors from the assembly function into a single vector embedding for the function. Much like Asm2Vec, SAFE requires
pre-training, but can perform cross architecture similarity detection.

Order Matters [16] applies a BERT language reprensentation model [15] along with control flow graph analysis
to perform BCSD. It uses BERT to learn the embeddings of instructions and basic blocks from the function,
passes the CFG through a graph neural network to obtain a graph semantic embedding, and sends the adjacency
matrix of the CFG through a convolutional neural network to compute a graph order embedding. These embeddings
are then combined using a multi-layer perceptron, obtaining the assembly function's embedding. 

A more recent BCSD model, PalmTree [14], also bases its work on the BERT model [15].
It considers each instruction as a sentence, and decomposes it into basic tokens. The model is trained
on three tasks. 1. As is common for BERT models, PalmTree is trained on masked language modeling. 2.
PalmTree is trained on context window prediction, that is predicting whether two instructions are found
in the same context window of an assembly function. 3. The model is also trained on Def-Use Prediction -
predicting whether there is a definition-usage relation between both instructions.

The model CLAP [19] uses the RoBERTa [20] base model, to perform assembly function encoding.
It is adapted for assembly instruction tokenization, and directly generates an embedding for
a whole assembly function. It is also accompanied with a text encoder (CLAP-text), so that classification can
be performed using human readable classes. Categories or labels are encoded with the text encoder, and the
assembly function with the assembly encoder. The generated embeddings can be compared with cosine similarity to
calculate whether the assembly function is closely related or opposed to the category. This model requires
pre-training and is architecture specific (x86_64 compiled with GCC).

## Method

We use two distinct strategies to perform BCSD, and demonstrate that both methods target different aspects of the assembly fragment.
Our best results are obtained by combining both methods.

### Prompt

The first method consists of querying a large language model with a prompt crafted to extract the high-level behavioral features of
the provided assembly code fragment. The assembly code fragment does not require preprocessing. As output, the LLM generates a JSON
structure containing the extracted features.

#### Framing and conditioning

We use a prelude that contains general information about the task at hand, and the expected response.
Recent commercial LLMs have the ability to generate responses following a specified JSON schema. We do not
make use of this capability when evaluating commercial models so that the results can be compared to local LLMs,
that do not benefit from this option.

> You are an expert assembly code analyst, specializing in high-level semantic description and feature extraction for comparative analysis. Your goal is to analyze an assembly routine from an **unspecified architecture and compiler** and provide its extracted high-level features, formatted as a JSON object.
For the provided assembly routine, extract the following features and infer the algorithm. Your output **MUST** be a JSON object conforming to the structure
defined by these features.

#### Signature and data flow

The first high-level feature category consists of analyzing the type signature of the provided assembly function.
The following elements are considered:

#### Core logic & Operations

This section specifies how to determine the kind of operation that the assembly function performs.

The information collected includes:

- Indication of loops. This is determined by the presence of jump instructions that point back to a
    previous instruction after some conditions have been checked.
- Indication of jump tables. Evaluated by patterns suggesting calculated jump addresses based on
    indices, or a series of conditional jumps.
- Extensive use of indexed addressing modes.
- Use of SIMD instructions and registers.
- Number of distinct subroutine call targets.
- Overall logical behavior. Possibilities include:
  - Arithmetic operations
  - Bitwise operations
  - Data movement and memory access.
  - Control flow and dispatching operations.
  - Memory access operations.

#### Notable constants

This section identifies notable constants. These could be common scalar values used by a specific
cryptographic algorithm, or the signature bytes used by a file format or protocol.

#### Side effects

The prompt also monitors the side effects that the assembly function has on the system. This includes:

- Modification of input arguments.
- Modification of global state.
    This is detected when writes to absolute memory addresses or addresses resolved via global data segment pointers occur.
- Memory allocation and deallocation.
    Detected by the presence of calls to memory management functions like `malloc`, `free`, or similar.
- Linear memory access patterns.
    Determined by the presence of sequential indexed memory accesses inside loops or across multiple instructions.
- System calls and software interrupts.
    This is identified by the presence of specific instructions that trigger system calls or software interrupts.

#### Final categorization

The last section tries to assign a overall category to the assembly function, by basing it on the information
collected in the analysis. The final categorization only weakly supports the similarity search because it does
not have a large impact on the similarity score. Its purpose is to provide a concise overview for reviewers
of the analysis, who might want to understand the function or verify its similarity with the target.
Categories include: cryptographic, data processing, control flow and dispatch, initialization, error handling,
wrapper/utility, file management, etc.
. First,  

To perform BCSD, our method queries an LLM with an assembly function in its context window to generate an
analysis of the function. We compare two methods to obtain a successful assembly analysis.
1. We use a system prompt that specifies the analysis to be performed.
2. We use few-shot prompting and provide a few manually crafted user-assistant interactions as context and ommit the system
    prompt entirely.

For BCSD, we mandate that the generated response follows a specified JSON schema. These JSON responses can be compared
to calculate a numerical index of similarity.

### Comparison

Machine learning based methods of BCSD typically generate a numerical vector embedding for each assembly function [refs], and then compare these vectors
using numerical methods such as cosine similarity. This method is different, because it generates a JSON object instead of a vector. This has
the unique advantage of being human interpretable, and thus easier to verify. For example, when matching a binary function against a database,
one can base themself on the human readable LLM output to verify that the analysis was done correctly and to assess whether the matched function
is indeed a clone.

Since our generated analysis is not numerical, we use an alternative method to compare two assembly functions. We interpret the JSON structure
as a tree, where booleans, numbers, and string elements are leaves, and non-empty lists and objects fields are nodes. We form a set containing all
root-to-leaf paths, and use jaccard similarity (Intesection over union) to obtain a similarity measure.

QUESTION: Do we need refs for cosine similarity & jaccard?

### Dataset

The dataset is composed of 7 binaries: busybox, coreutils, curl, image-magick, openssl, putty, and sqlite3.
All were compiled using gcc for the following platforms: x86_64, x86_32, arm, mips, powerpc.
For each binary and platform, binary objects were generated for all optimization levels (O0 to O3),
stripped of debug symbols. In total, yeilds 140 different binaries to analyze.
The binaries were dissassembled using IDA Pro, yielding 383_658 assembly functions.
Functions consisting of less than 3 instructions were not included as part of the dataset.
Pairs of equivalent functions from the same platform but distinct optimization level were made for cross optimization
evaluation, and pairs from the same optimization level but different platform were formed for cross
platform evaluation.

### Model

These local models are evaluated on our dataset.

- Qwen2.5-Coder [ref] with sizes 0.5B to 7B
- Qwen3 [ref] with sizes 0.6 to 4B
- Gemma-3n [ref] with sizes 0.5B to 4B

Most evaluations and tests were run using Qwen2.5-Coder, and we use this model as a baseline.
On all local models, the input context size was limited to 4096 tokens, and output tokens generation to 512.
Large assembly functions that did not fit within the input tokens were truncated.

For ablation study, the following commercial models were evaluated on our dataset.

- OpenAI's GPT-4.1-mini [ref]
- OpenAI's o4-mini [ref]
- Google's gemini-2.5-flash-lite [ref]

### Prompt

- What are the features we extract?
- What type of output do we expect (main different between prompts)?
    - How do we solve repetition.
    - How do we ensure a valid json output?

### Evaluation method

The mean reciprocal rank (MRR) and first position recall (Recall@1) metrics are used for evaluation and comparison to other methods.
A pool of 1_000 function pairs is used for evaluation. For each pair, we compare the generated features for the first element of the pair
with all second elements of the pairs contained in the pool. We rank the comparisons in order of most similar to least similar, and
calculate the metrics based on this ranking.

## Results

- Show results on cross-optimization and cross-architecture (Many different models ran locally).
- Ablation on the number of examples provided.
- Ablation on the model size.
- Ablation on the prompt used, examples of output.
- Show results on commercial models
- Token usage on commercial models

### Human interpretability

Our method offers a distinct advantage over the current state-of-the art, because the feature vectors generated are human interpretable.
Other machine learning based methods generate numerical vector embeddings and compare the vectors using numerical methods such as cosine similarity.

## Future Research

These results open avenues for further investigation in LLM based BCSD, and more broadly in LLM assisted reverse engineering.
A few of these oportunities are outlined here.

### Distillation & Fine Tuning

It was shown that commercial LLMs perform better than smaller local models. Using the process of distillation [ref] on smaller models
will likely cause large gains for these models, approaching the performance of foundational LLMs. Fine Tuning is another opportunity
that should be explored to determine whether it provides meaningful gains in performance. One method would be using open source software
as fine tuning data. One could use source code and generate a feature vector (either manually or by prompting a large language model),
and then use this analysis as baseline to fine tune the analysis of the compiled assembly code.

### Comparison as opposed to function embedding generation

TODO

### LLM Reasoning

Some state of the art LLMs provide reasoning capabilities by having the model generate a chain of thought [8] it its response.
It has been shown that this type of output produces more accurate and higher quality responses [8, 9, 10]. We believe that similar
gains could be seen in assembly function analysis, at the cost of more output token generation and thus more compute.

# Figures, images & graphs

- Diff example for generated json output vs. diff for assembly function.
- 


# Refs

1. [A Survey of Binary Code Similarity Detection Techniques](https://www.mdpi.com/2079-9292/13/9/1715)
2. [Binary Code Similiarity Detection](https://ieeexplore.ieee.org/document/9678518)
3. [Scalable Graph-based Bug Search for Firmware Images](https://dl.acm.org/doi/10.1145/2976749.2978370)
4. [Graph-based comparison of Executable Objects](https://www.semanticscholar.org/paper/Graph-based-comparison-of-Executable-Objects-Dullien/7661d4110ef24dea74190f4af69bd206d6253db9)

5. [Detecting Clones Across Microsoft .NET Programming Languages](https://ieeexplore.ieee.org/document/6385136)
6. [Idea: Opcode-Sequence-Based Malware Detection](https://link.springer.com/chapter/10.1007/978-3-642-11747-3_3)
7. [Binary Function Clustering Using Semantic Hashes](https://ieeexplore.ieee.org/document/6406693)

8. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
9. [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916)
10. [THINKING LLMS: GENERAL INSTRUCTION FOLLOWING WITH THOUGHT GENERATION](https://arxiv.org/pdf/2410.10630)

11. [SAFE: Self-Attentive Function Embeddings for Binary Similarity](https://arxiv.org/pdf/1811.05296)
12. [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546)
13. [A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING](https://arxiv.org/pdf/1703.03130)

14. [PalmTree: Learning an Assembly Language Model for Instruction Embedding](https://arxiv.org/pdf/2103.03809)
15. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)

16. [Order Matters: Semantic-Aware Neural Networks for Binary Code Similarity Detection](https://cdn.aaai.org/ojs/5466/5466-13-8691-1-10-20200511.pdf)

17. [Asm2Vec: Boosting Static Representation Robustness for Binary Clone Search against Code Obfuscation and Compiler Optimization](https://ieeexplore.ieee.org/document/8835340)
18. [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053)

19. [CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision](https://dl.acm.org/doi/pdf/10.1145/3650212.3652145)
20. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)

- [Blanket execution: Dynamic similarity testing for program binaries and components](https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/egele)
- [BinSim: Trace-based Semantic Binary Diffing via System Call Sliced Segment Equivalence Checking](https://www.usenix.org/system/files/conference/usenixsecurity17/sec17-ming.pdf)